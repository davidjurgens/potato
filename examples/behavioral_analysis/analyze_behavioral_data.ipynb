{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Data Analysis for Potato Annotations\n",
    "\n",
    "This notebook demonstrates how to analyze behavioral tracking data collected by Potato's interaction tracking system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Potato tracks:\n",
    "- **Interactions**: Clicks, focus changes, keyboard shortcuts, navigation\n",
    "- **AI Assistance**: Request/response timing, acceptance rates\n",
    "- **Annotation Changes**: All modifications with timestamps and sources\n",
    "- **Timing Data**: Session duration, focus time per element, scroll depth\n",
    "\n",
    "This data enables:\n",
    "- Quality assessment of annotations\n",
    "- Identification of problematic annotators\n",
    "- Analysis of AI assistance effectiveness\n",
    "- Optimization of annotation workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Optional: for visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_PLOTS = True\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = [10, 6]\n",
    "except ImportError:\n",
    "    HAS_PLOTS = False\n",
    "    print(\"matplotlib/seaborn not available - skipping visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Behavioral Data\n",
    "\n",
    "Behavioral data can be loaded from:\n",
    "1. Individual user state files (`annotation_output/<user>/user_state.json`)\n",
    "2. Example data files (like the one in this directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_behavioral_data_from_file(filepath: str) -> dict:\n",
    "    \"\"\"Load behavioral data from a JSON file.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_behavioral_data_from_annotation_output(output_dir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load behavioral data from Potato annotation output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Path to annotation_output directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping user_id -> instance_id -> behavioral_data\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    for user_dir in output_path.iterdir():\n",
    "        if not user_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        state_file = user_dir / 'user_state.json'\n",
    "        if state_file.exists():\n",
    "            with open(state_file) as f:\n",
    "                user_state = json.load(f)\n",
    "            \n",
    "            user_id = user_state.get('user_id', user_dir.name)\n",
    "            behavioral = user_state.get('instance_id_to_behavioral_data', {})\n",
    "            if behavioral:\n",
    "                data[user_id] = behavioral\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load example data\n",
    "behavioral_data = load_behavioral_data_from_file('example_behavioral_data.json')\n",
    "print(f\"Loaded data for {len(behavioral_data)} users\")\n",
    "for user_id, instances in behavioral_data.items():\n",
    "    print(f\"  {user_id}: {len(instances)} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "\n",
    "Let's calculate some basic statistics about annotation behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_stats(behavioral_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate per-user statistics from behavioral data.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - user_id, total_instances, total_time_sec, avg_time_sec,\n",
    "    - total_interactions, avg_interactions, total_changes, avg_changes,\n",
    "    - ai_requests, ai_accepts, ai_accept_rate\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        times = []\n",
    "        interactions = []\n",
    "        changes = []\n",
    "        ai_requests = 0\n",
    "        ai_accepts = 0\n",
    "        \n",
    "        for instance_id, bd in instances.items():\n",
    "            # Time\n",
    "            time_ms = bd.get('total_time_ms', 0)\n",
    "            times.append(time_ms / 1000)\n",
    "            \n",
    "            # Interactions\n",
    "            interactions.append(len(bd.get('interactions', [])))\n",
    "            \n",
    "            # Changes\n",
    "            changes.append(len(bd.get('annotation_changes', [])))\n",
    "            \n",
    "            # AI usage\n",
    "            for ai in bd.get('ai_usage', []):\n",
    "                ai_requests += 1\n",
    "                if ai.get('suggestion_accepted'):\n",
    "                    ai_accepts += 1\n",
    "        \n",
    "        rows.append({\n",
    "            'user_id': user_id,\n",
    "            'total_instances': len(instances),\n",
    "            'total_time_sec': sum(times),\n",
    "            'avg_time_sec': np.mean(times) if times else 0,\n",
    "            'min_time_sec': min(times) if times else 0,\n",
    "            'max_time_sec': max(times) if times else 0,\n",
    "            'total_interactions': sum(interactions),\n",
    "            'avg_interactions': np.mean(interactions) if interactions else 0,\n",
    "            'total_changes': sum(changes),\n",
    "            'avg_changes': np.mean(changes) if changes else 0,\n",
    "            'ai_requests': ai_requests,\n",
    "            'ai_accepts': ai_accepts,\n",
    "            'ai_accept_rate': ai_accepts / ai_requests if ai_requests > 0 else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "user_stats = calculate_user_stats(behavioral_data)\n",
    "user_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotation Time Analysis\n",
    "\n",
    "Analyze how long annotators spend on each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_annotation_times(behavioral_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract annotation times for all instances.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        for instance_id, bd in instances.items():\n",
    "            rows.append({\n",
    "                'user_id': user_id,\n",
    "                'instance_id': instance_id,\n",
    "                'time_sec': bd.get('total_time_ms', 0) / 1000,\n",
    "                'interactions': len(bd.get('interactions', [])),\n",
    "                'changes': len(bd.get('annotation_changes', [])),\n",
    "                'scroll_depth': bd.get('scroll_depth_max', 0)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "times_df = get_all_annotation_times(behavioral_data)\n",
    "print(\"Annotation Time Statistics:\")\n",
    "print(times_df['time_sec'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PLOTS:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram of annotation times\n",
    "    axes[0].hist(times_df['time_sec'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Time (seconds)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Distribution of Annotation Times')\n",
    "    axes[0].axvline(times_df['time_sec'].mean(), color='red', linestyle='--', label=f'Mean: {times_df[\"time_sec\"].mean():.1f}s')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot by user\n",
    "    times_df.boxplot(column='time_sec', by='user_id', ax=axes[1])\n",
    "    axes[1].set_xlabel('User ID')\n",
    "    axes[1].set_ylabel('Time (seconds)')\n",
    "    axes[1].set_title('Annotation Time by User')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI Assistance Analysis\n",
    "\n",
    "Analyze how annotators use AI assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ai_usage(behavioral_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze AI assistance usage patterns.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with AI usage statistics\n",
    "    \"\"\"\n",
    "    ai_events = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        for instance_id, bd in instances.items():\n",
    "            for ai in bd.get('ai_usage', []):\n",
    "                ai_events.append({\n",
    "                    'user_id': user_id,\n",
    "                    'instance_id': instance_id,\n",
    "                    'schema': ai.get('schema_name'),\n",
    "                    'suggestions': ai.get('suggestions_shown', []),\n",
    "                    'accepted': ai.get('suggestion_accepted'),\n",
    "                    'decision_time_ms': ai.get('time_to_decision_ms'),\n",
    "                    'response_latency': (\n",
    "                        ai.get('response_timestamp', 0) - ai.get('request_timestamp', 0)\n",
    "                    ) if ai.get('response_timestamp') else None\n",
    "                })\n",
    "    \n",
    "    if not ai_events:\n",
    "        return {'total_requests': 0, 'message': 'No AI usage data found'}\n",
    "    \n",
    "    df = pd.DataFrame(ai_events)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_requests = len(df)\n",
    "    total_accepts = df['accepted'].notna().sum()\n",
    "    total_rejects = total_requests - total_accepts\n",
    "    \n",
    "    decision_times = df['decision_time_ms'].dropna()\n",
    "    \n",
    "    return {\n",
    "        'total_requests': total_requests,\n",
    "        'total_accepts': total_accepts,\n",
    "        'total_rejects': total_rejects,\n",
    "        'accept_rate': total_accepts / total_requests if total_requests > 0 else 0,\n",
    "        'avg_decision_time_ms': decision_times.mean() if len(decision_times) > 0 else None,\n",
    "        'by_user': df.groupby('user_id').agg({\n",
    "            'accepted': lambda x: x.notna().sum(),\n",
    "            'decision_time_ms': 'mean'\n",
    "        }).to_dict(),\n",
    "        'events_df': df\n",
    "    }\n",
    "\n",
    "ai_analysis = analyze_ai_usage(behavioral_data)\n",
    "print(\"AI Assistance Analysis:\")\n",
    "print(f\"  Total requests: {ai_analysis['total_requests']}\")\n",
    "print(f\"  Accepts: {ai_analysis['total_accepts']}\")\n",
    "print(f\"  Rejects: {ai_analysis['total_rejects']}\")\n",
    "print(f\"  Accept rate: {ai_analysis['accept_rate']:.1%}\")\n",
    "if ai_analysis.get('avg_decision_time_ms'):\n",
    "    print(f\"  Avg decision time: {ai_analysis['avg_decision_time_ms']:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Pattern Analysis\n",
    "\n",
    "Analyze what elements users interact with and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_interactions(behavioral_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze interaction patterns.\"\"\"\n",
    "    interactions = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        for instance_id, bd in instances.items():\n",
    "            for event in bd.get('interactions', []):\n",
    "                interactions.append({\n",
    "                    'user_id': user_id,\n",
    "                    'instance_id': instance_id,\n",
    "                    'event_type': event.get('event_type'),\n",
    "                    'target': event.get('target'),\n",
    "                    'timestamp': event.get('timestamp')\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "interactions_df = analyze_interactions(behavioral_data)\n",
    "\n",
    "print(\"\\nInteraction Event Types:\")\n",
    "print(interactions_df['event_type'].value_counts())\n",
    "\n",
    "print(\"\\nMost Common Targets:\")\n",
    "print(interactions_df['target'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PLOTS:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Event types\n",
    "    event_counts = interactions_df['event_type'].value_counts()\n",
    "    event_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "    axes[0].set_title('Interaction Event Types')\n",
    "    axes[0].set_xlabel('Event Type')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Target categories\n",
    "    interactions_df['target_category'] = interactions_df['target'].apply(\n",
    "        lambda x: x.split(':')[0] if ':' in str(x) else str(x)\n",
    "    )\n",
    "    target_counts = interactions_df['target_category'].value_counts()\n",
    "    target_counts.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
    "    axes[1].set_title('Interaction Target Categories')\n",
    "    axes[1].set_xlabel('Target Category')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quality Detection: Finding Suspicious Annotators\n",
    "\n",
    "Identify annotators who may be providing low-quality annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_suspicious_behavior(behavioral_data: dict,\n",
    "                               min_time_threshold: float = 5.0,\n",
    "                               min_interactions_threshold: int = 3,\n",
    "                               min_scroll_threshold: float = 25.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identify annotators with potentially low-quality behavior.\n",
    "    \n",
    "    Flags:\n",
    "    - Very fast annotation times (< min_time_threshold seconds)\n",
    "    - Very few interactions per instance\n",
    "    - No scroll activity (didn't read the text)\n",
    "    - No annotation changes (just clicking through)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with suspicious behavior indicators per user\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        fast_count = 0\n",
    "        low_interaction_count = 0\n",
    "        no_scroll_count = 0\n",
    "        no_change_count = 0\n",
    "        total = len(instances)\n",
    "        \n",
    "        for instance_id, bd in instances.items():\n",
    "            time_sec = bd.get('total_time_ms', 0) / 1000\n",
    "            interactions = len(bd.get('interactions', []))\n",
    "            changes = len(bd.get('annotation_changes', []))\n",
    "            scroll = bd.get('scroll_depth_max', 0)\n",
    "            \n",
    "            if time_sec < min_time_threshold:\n",
    "                fast_count += 1\n",
    "            if interactions < min_interactions_threshold:\n",
    "                low_interaction_count += 1\n",
    "            if scroll < min_scroll_threshold:\n",
    "                no_scroll_count += 1\n",
    "            if changes == 0:\n",
    "                no_change_count += 1\n",
    "        \n",
    "        if total > 0:\n",
    "            fast_rate = fast_count / total\n",
    "            low_rate = low_interaction_count / total\n",
    "            no_scroll_rate = no_scroll_count / total\n",
    "            no_change_rate = no_change_count / total\n",
    "            \n",
    "            # Calculate suspicion score (0-1)\n",
    "            suspicion_score = (\n",
    "                fast_rate * 0.3 + \n",
    "                low_rate * 0.2 + \n",
    "                no_scroll_rate * 0.2 + \n",
    "                no_change_rate * 0.3\n",
    "            )\n",
    "            \n",
    "            rows.append({\n",
    "                'user_id': user_id,\n",
    "                'total_instances': total,\n",
    "                'fast_annotation_rate': fast_rate,\n",
    "                'low_interaction_rate': low_rate,\n",
    "                'no_scroll_rate': no_scroll_rate,\n",
    "                'no_change_rate': no_change_rate,\n",
    "                'suspicion_score': suspicion_score,\n",
    "                'flag': 'SUSPICIOUS' if suspicion_score > 0.5 else 'OK'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows).sort_values('suspicion_score', ascending=False)\n",
    "\n",
    "suspicious_df = detect_suspicious_behavior(behavioral_data)\n",
    "print(\"Annotator Quality Analysis:\")\n",
    "suspicious_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PLOTS:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    metrics = ['fast_annotation_rate', 'low_interaction_rate', 'no_scroll_rate', 'no_change_rate']\n",
    "    x = np.arange(len(suspicious_df))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax.bar(x + i * width, suspicious_df[metric], width, label=metric.replace('_', ' ').title())\n",
    "    \n",
    "    ax.set_xlabel('User')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('Quality Indicators by User')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(suspicious_df['user_id'])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Focus Time Analysis\n",
    "\n",
    "Analyze where annotators spend their time looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_focus_time(behavioral_data: dict) -> dict:\n",
    "    \"\"\"Analyze focus time distribution across elements.\"\"\"\n",
    "    focus_totals = defaultdict(int)\n",
    "    focus_by_user = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        for instance_id, bd in instances.items():\n",
    "            for element, time_ms in bd.get('focus_time_by_element', {}).items():\n",
    "                focus_totals[element] += time_ms\n",
    "                focus_by_user[user_id][element] += time_ms\n",
    "    \n",
    "    return {\n",
    "        'totals': dict(focus_totals),\n",
    "        'by_user': {k: dict(v) for k, v in focus_by_user.items()}\n",
    "    }\n",
    "\n",
    "focus_analysis = analyze_focus_time(behavioral_data)\n",
    "print(\"\\nTotal Focus Time by Element (ms):\")\n",
    "for element, time_ms in sorted(focus_analysis['totals'].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {element}: {time_ms}ms ({time_ms/1000:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Annotation Change Patterns\n",
    "\n",
    "Analyze how annotators change their minds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_annotation_changes(behavioral_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze annotation change patterns.\"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    for user_id, instances in behavioral_data.items():\n",
    "        for instance_id, bd in instances.items():\n",
    "            instance_changes = bd.get('annotation_changes', [])\n",
    "            \n",
    "            for change in instance_changes:\n",
    "                changes.append({\n",
    "                    'user_id': user_id,\n",
    "                    'instance_id': instance_id,\n",
    "                    'schema': change.get('schema_name'),\n",
    "                    'label': change.get('label_name'),\n",
    "                    'action': change.get('action'),\n",
    "                    'source': change.get('source', 'user'),\n",
    "                    'timestamp': change.get('timestamp')\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "changes_df = analyze_annotation_changes(behavioral_data)\n",
    "\n",
    "print(\"\\nAnnotation Change Actions:\")\n",
    "print(changes_df['action'].value_counts())\n",
    "\n",
    "print(\"\\nAnnotation Change Sources:\")\n",
    "print(changes_df['source'].value_counts())\n",
    "\n",
    "# Mind changes: select followed by deselect\n",
    "mind_changes = changes_df.groupby(['user_id', 'instance_id']).apply(\n",
    "    lambda x: (x['action'] == 'deselect').sum()\n",
    ").reset_index(name='mind_changes')\n",
    "\n",
    "print(\"\\nMind Changes (deselections) by User:\")\n",
    "print(mind_changes.groupby('user_id')['mind_changes'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generating Reports\n",
    "\n",
    "Create summary reports for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(behavioral_data: dict) -> str:\n",
    "    \"\"\"Generate a text summary report of behavioral data.\"\"\"\n",
    "    user_stats = calculate_user_stats(behavioral_data)\n",
    "    ai_stats = analyze_ai_usage(behavioral_data)\n",
    "    suspicious = detect_suspicious_behavior(behavioral_data)\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 60)\n",
    "    report.append(\"BEHAVIORAL DATA ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 60)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Overview\n",
    "    report.append(\"OVERVIEW\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Total Annotators: {len(user_stats)}\")\n",
    "    report.append(f\"Total Instances Annotated: {user_stats['total_instances'].sum()}\")\n",
    "    report.append(f\"Total Annotation Time: {user_stats['total_time_sec'].sum() / 60:.1f} minutes\")\n",
    "    report.append(f\"Average Time per Instance: {user_stats['avg_time_sec'].mean():.1f} seconds\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # AI Usage\n",
    "    report.append(\"AI ASSISTANCE USAGE\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Total AI Requests: {ai_stats['total_requests']}\")\n",
    "    report.append(f\"Suggestions Accepted: {ai_stats['total_accepts']}\")\n",
    "    report.append(f\"Accept Rate: {ai_stats['accept_rate']:.1%}\")\n",
    "    if ai_stats.get('avg_decision_time_ms'):\n",
    "        report.append(f\"Avg Decision Time: {ai_stats['avg_decision_time_ms']:.0f}ms\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Quality\n",
    "    report.append(\"QUALITY INDICATORS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    suspicious_users = suspicious[suspicious['flag'] == 'SUSPICIOUS']\n",
    "    report.append(f\"Flagged Annotators: {len(suspicious_users)} of {len(suspicious)}\")\n",
    "    if len(suspicious_users) > 0:\n",
    "        report.append(f\"Flagged Users: {', '.join(suspicious_users['user_id'].tolist())}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Per-user summary\n",
    "    report.append(\"PER-USER SUMMARY\")\n",
    "    report.append(\"-\" * 40)\n",
    "    for _, row in user_stats.iterrows():\n",
    "        flag = suspicious[suspicious['user_id'] == row['user_id']]['flag'].values[0]\n",
    "        report.append(f\"\\n{row['user_id']} [{flag}]:\")\n",
    "        report.append(f\"  Instances: {row['total_instances']}\")\n",
    "        report.append(f\"  Avg Time: {row['avg_time_sec']:.1f}s\")\n",
    "        report.append(f\"  Avg Interactions: {row['avg_interactions']:.1f}\")\n",
    "        if row['ai_requests'] > 0:\n",
    "            report.append(f\"  AI Accept Rate: {row['ai_accept_rate']:.1%}\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "print(generate_summary_report(behavioral_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Save analysis results for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "user_stats.to_csv('user_statistics.csv', index=False)\n",
    "suspicious_df.to_csv('quality_analysis.csv', index=False)\n",
    "times_df.to_csv('annotation_times.csv', index=False)\n",
    "interactions_df.to_csv('interactions.csv', index=False)\n",
    "changes_df.to_csv('annotation_changes.csv', index=False)\n",
    "\n",
    "print(\"Exported files:\")\n",
    "print(\"  - user_statistics.csv\")\n",
    "print(\"  - quality_analysis.csv\")\n",
    "print(\"  - annotation_times.csv\")\n",
    "print(\"  - interactions.csv\")\n",
    "print(\"  - annotation_changes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Integration with Admin Dashboard**: View real-time analytics at `/admin`\n",
    "2. **Custom Thresholds**: Adjust detection thresholds based on your task\n",
    "3. **Longitudinal Analysis**: Track behavior changes over time\n",
    "4. **Correlation Analysis**: Compare behavioral data with annotation quality scores\n",
    "\n",
    "For more information, see:\n",
    "- [Behavioral Tracking Documentation](../../docs/behavioral_tracking.md)\n",
    "- [Admin Dashboard Documentation](../../docs/admin_dashboard.md)\n",
    "- [Quality Control Documentation](../../docs/quality_control.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
